{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volatility Analysis\n",
    "This notebook adresses Chapter 1 of our research project, namely analyzing the stability of stock volatility (standard deviation of returns) over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import wrds\n",
    "import pandas_market_calendars as mcal\n",
    "from datetime import date, timedelta, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "conn = wrds.Connection(wrds_username='kaihvc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "The goal of this section is to generate a dataset of weekly returns (Fri-Fri) for each stock in our universe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying WRDS for Daily Returns\n",
    "We query the WRDS CRSP stock database for daily returns for any stock with a 2020 $10mil market cap (adjusted for inflation using the GDP deflator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRDS database request & processing\n",
    "# build SQL query using inflation-adjusted mcaps\n",
    "\n",
    "# load inflation-adjusted market caps from file (equivalents of 2020 $10mil)\n",
    "def get_cutoffs(filename):\n",
    "    cutoffs = pd.read_csv(filename)[['observation_date', 'Cutoff Value']]\n",
    "    cutoffs['year'] = pd.to_datetime(cutoffs['observation_date']).apply(lambda x: x.year)\n",
    "    cutoffs = cutoffs.set_index('year').drop(columns='observation_date')\n",
    "    return cutoffs\n",
    "\n",
    "# build query, filtering using inflation-adjusted mcap values\n",
    "def build_query(start_dt, end_dt, adjusted_mcaps):\n",
    "    query = f\"SELECT cusip, date, ret FROM crsp.dsf WHERE date>='{str(start_dt)}' AND date<='{str(end_dt)}' AND (\"\n",
    "    for i, year in enumerate(adjusted_mcaps.loc[start_dt.year:].index):\n",
    "        query += f\"(EXTRACT(YEAR FROM date)={year} AND prc*shrout>={int(adjusted_mcaps.loc[year][0] / 1000)})\"\n",
    "        query += \" OR \" if i < len(adjusted_mcaps.loc[start_dt.year:].index) - 1 else \")\"\n",
    "    return query\n",
    "        \n",
    "# get daily returns\n",
    "def get_returns(start_dt, end_dt, adjusted_mcaps, username='kaihvc'):\n",
    "    conn = wrds.Connection(wrds_username=username)\n",
    "    query = build_query(start_dt, end_dt, adjusted_mcaps)\n",
    "    df = conn.raw_sql(query)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "# we'll use the 1950 inflation-adjusted equivalent of a 2020 $10mil mcap minimum, and do more fine-grained filtering later\n",
    "start_dt = date(1950, 1, 1)\n",
    "end_dt = date(2021, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query returns\n",
    "requery = False\n",
    "if requery:\n",
    "    exec_start = datetime.now()\n",
    "    print(\"Querying WRDS for daily returns...\")\n",
    "    daily_rets = get_returns(start_dt, end_dt, get_cutoffs('inflation_adjustments.csv'))\n",
    "    print(f\"Query complete in {str(datetime.now() - exec_start)}, writing to disk...\")\n",
    "    stage_start = datetime.now()\n",
    "    daily_rets.to_csv('wrds_results/daily_rets_adjusted.csv')\n",
    "    print(f\"Disk write complete in {str(datetime.now() - stage_start)}\")\n",
    "    print(f\"Download complete in {str(datetime.now() - exec_start)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Daily Data to Weekly Data\n",
    "To convert our daily data into weekly data, we'll neeed to delineate the weeks (excluding market holidays, defining minimum days, etc.). We'll then geometrically expand each week's daily returns ($r_d$ for each day $d$) to get the weekly return $r_w$ as:  \n",
    "$r_w = ((1 + r_1) \\cdot (1 + r_2) \\cdot (1 + r_3) \\cdot (1 + r_4) \\cdot (1 + r_5)) - 1$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_process_weekly(df, start_dt, end_dt):\n",
    "    \n",
    "    # takes ~25-30 min on my machine (16G, 2.8GHz) all told; most spent on the aggregation\n",
    "    # group by cusip & weeks, then geometrically expand week groups to get weekly returns\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    grouper = pd.Grouper(key='date', freq='W-MON')\n",
    "    grouped = df.groupby(by=['cusip', grouper], observed=True)\n",
    "    weekly_rets = grouped.agg({'ret': lambda x: (x + 1).product() - 1})\n",
    "    \n",
    "    return weekly_rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate returns\n",
    "load = False\n",
    "if load:\n",
    "    recalc = False\n",
    "    if recalc:\n",
    "        if 'daily_rets' not in globals():\n",
    "            print(\"Loading daily returns from file...\")\n",
    "            daily_rets = pd.read_csv('wrds_results/daily_rets_adjusted.csv', index_col=0)\n",
    "        exec_start = datetime.now()\n",
    "        weekly_rets = pd_process_weekly(daily_rets, start_dt, end_dt)\n",
    "        print(f\"Processing complete in {str(datetime.now() - exec_start)}, writing to disk...\")\n",
    "        weekly_rets.to_csv('wrds_results/weekly_rets_adjusted.csv')\n",
    "        print(f'Weekly returns downloaded in {str(datetime.now() - exec_start)}')\n",
    "    else:\n",
    "        weekly_rets = pd.read_csv('wrds_results/weekly_rets_adjusted.csv', index_col=[0, 1])\n",
    "        weekly_rets.index = weekly_rets.index.set_levels([weekly_rets.index.levels[0], pd.to_datetime(weekly_rets.index.levels[1])])\n",
    "        print('Weekly returns loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "Once we have weekly returns, we want to calculate their standard deviations over a ~6-month period for each stock, which will be our primary source of data on the stocks' volatility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating 6mo. Standard Deviations\n",
    "Calculating standard deviations is relatively straightforward; the main caveat is generating the periods. We'll be using periods of _approximately_ 26 weeks, with occasional corrections to avoid drift (we want to start as close to the first trading week of the year, or the 26th week of the year, as we can)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_days(start_dt, end_dt):\n",
    "    nyse = mcal.get_calendar('NYSE')\n",
    "    return [day.date() for day in nyse.valid_days(start_date=str(start_dt), \n",
    "                                                end_date=str(end_dt))]\n",
    "\n",
    "def get_weeks(start_dt, end_dt):\n",
    "    weeks = []\n",
    "\n",
    "    # get trading days and divide into weeks, rolling over on the given rollover day\n",
    "    current_week = []\n",
    "    days = get_days(start_dt, end_dt)\n",
    "    for i, day in enumerate(days):\n",
    "        # append to current week\n",
    "        current_week.append(day)\n",
    "\n",
    "        #end of the week check\n",
    "        if i+1<len(days):\n",
    "            next_day = days[i+1]\n",
    "            if next_day.weekday() < day.weekday(): #if the next day's weekday comes before the current, then the next_day is in a new week\n",
    "                weeks.append(current_week)\n",
    "                current_week = []\n",
    "            elif (next_day-day).days > 4: #if the condition above didnt pass, check that >4 days didnt pass as well\n",
    "                weeks.append(current_week)\n",
    "                current_week = []\n",
    "\n",
    "    # append final week, even if it wasn't a full week period\n",
    "    if current_week: weeks.append(current_week)\n",
    "\n",
    "    return weeks\n",
    "\n",
    "def get_periods(weeks):\n",
    "    periods = []\n",
    "    start_date = weeks[0][0]\n",
    "    end_date = None\n",
    "    for i in range(len(weeks)-1):\n",
    "        start = weeks[i][0] #this monday\n",
    "        end = weeks[i][-1]  #this friday\n",
    "\n",
    "        next_start = weeks[i+1][0] #next monday\n",
    "        next_end = weeks[i+1][-1]  #next friday\n",
    "\n",
    "        #if the current friday is in december, and the next weeks friday is in january, this is the end of one period\n",
    "        #the very next week will be the very start of the period\\\n",
    "        if end.month == 12 and next_end.month == 1:\n",
    "            end_date = end\n",
    "            periods.append([start_date,end_date])\n",
    "            start_date = next_start\n",
    "        #if the last day is in june, and the next weeks last day is in july, this is the end of one period\n",
    "        #the very next week will be the very start of the period\n",
    "        if end.month == 6 and next_end.month == 7:\n",
    "            end_date = end\n",
    "            periods.append([start_date,end_date])\n",
    "            start_date = next_start\n",
    "    periods.append([start_date, weeks[i+1][-1]])\n",
    "    for i in periods:\n",
    "        i[1] += timedelta(days=3)\n",
    "    return periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate standard deviations of ~26 week periods; \n",
    "# must have min_valid_weeks (default 20) out of 26 to be valid\n",
    "def get_6mo_stds(weekly_rets, periods, min_valid_weeks=20):\n",
    "    \n",
    "    num_pers = len(periods)\n",
    "    devs_df_list = []\n",
    "    devs = pd.DataFrame(index=weekly_rets.index.levels[0], columns=['P' + str(i) for i in range(len(periods))])\n",
    "\n",
    "    for i, per in enumerate(periods):\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        iter_start = datetime.now()\n",
    "\n",
    "        filter_counts = weekly_rets.loc[(slice(None), \n",
    "                                    slice(per[0], \n",
    "                                          per[1] - timedelta(days=1))), \n",
    "                                   :].groupby('cusip').count().rename(columns={'ret': 'count'})\n",
    "        agg_devs = weekly_rets.loc[(slice(None), \n",
    "                                    slice(per[0], \n",
    "                                          per[1] - timedelta(days=1))), \n",
    "                                   :].groupby('cusip').agg(np.std).loc[filter_counts['count'] >= min_valid_weeks]\n",
    "        devs.loc[agg_devs.index, 'P' + str(i)] = agg_devs.values.flatten()\n",
    "        devs_df_list.append(devs)\n",
    "                \n",
    "        iter_time = datetime.now() - iter_start\n",
    "        print(f'Cusip {i + 1}/{num_pers} complete in {str(iter_time)}')\n",
    "        print(f'Est. completion in {str(iter_time * (num_pers - (i + 1)))}')\n",
    "        \n",
    "    try:\n",
    "        devs = pd.concat(devs_df_list).astype(float)\n",
    "        return devs\n",
    "    except MemoryError:\n",
    "        print('Memory error encountered - attempting to remedy by dropping weekly_rets')\n",
    "        del weekly_rets\n",
    "        try:\n",
    "            devs = pd.concat(devs_df_list)\n",
    "            return devs\n",
    "        except MemoryError:\n",
    "            print('Remedy failed - reload weekly_rets and buy more memory!')\n",
    "            return -1\n",
    "    \n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviations loaded\n"
     ]
    }
   ],
   "source": [
    "recalc = False\n",
    "if recalc:\n",
    "    exec_start = datetime.now()\n",
    "    periods = get_periods(get_weeks(start_dt, end_dt))\n",
    "    df_devs = get_6mo_stds(weekly_rets, periods)\n",
    "    print('Writing to file...')\n",
    "    df_devs.to_csv('wrds_results/deviations.csv')\n",
    "    print(f'Done in {str(datetime.now() - exec_start)}')\n",
    "else:\n",
    "    df_devs = pd.read_csv('wrds_results/deviations.csv',index_col=0).astype(float)\n",
    "    print('Standard deviations loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking Stocks by Volatility\n",
    "Now that we have data for volatility, we want to rank it in order to perform some stability analysis on it. We'll do this using both straight deciles, as well as which portion of the distribution the standard error of each value falls in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deciles\n",
    "These are straight decile rankings - each ranking bin (1-10) contains 10% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discretize into 10 bins\n",
    "def discretize(df_devs, stderr=False, waiting_period=0):\n",
    "    \n",
    "    print('Beginning discretization...')\n",
    "    \n",
    "    if waiting_period > 0:\n",
    "        for i, row in df_devs.iterrows():\n",
    "            idxs = row.dropna().index[:waiting_period]\n",
    "            row[idxs] = np.nan\n",
    "            \n",
    "    df_devs = df_devs.dropna(axis='columns', how='all')\n",
    "    \n",
    "    df_disc = pd.DataFrame(columns=df_devs.columns)\n",
    "    num_cols = len(df_devs.columns)\n",
    "    labels = range(1, len(discretized.cat.categories) + 1)\n",
    "    for i, col in enumerate(df_devs.columns):\n",
    "        iter_start = datetime.now()\n",
    "        clear_output(wait=True)\n",
    "        if stderr:\n",
    "            intervals = [pd.Interval(elt, elt + 0.5) for elt in np.arange(-2, 2, 0.5)]\n",
    "            intervals = [pd.Interval(-np.inf, -2)] + intervals + [pd.Interval(2, np.inf)]    \n",
    "            intervals = pd.IntervalIndex(intervals)\n",
    "            \n",
    "            stderr_col = (df_devs[col] - np.nanmean(df_devs[col])) / np.nanstd(df_devs[col])\n",
    "\n",
    "            ''' \n",
    "                note: it's possible that not every period will include 10 categories, since this\n",
    "                bins by set intervals rather than quantiles of the data itself.\n",
    "            ''' \n",
    "            # all inline cause memory issues - save as much as we can, right?\n",
    "            vals = np.array(discretized = pd.cut(stderr_col, \n",
    "                                                 intervals, \n",
    "                                                 include_lowest=True).cat.rename_categories(labels))\n",
    "        else:\n",
    "            # however, this method *should* always include 10 categories for each period\n",
    "            vals = pd.qcut(df_devs[col], 10, labels=range(1, 11))\n",
    "        \n",
    "        df_disc[col] = vals\n",
    "        \n",
    "        iter_time = datetime.now() - iter_start\n",
    "        print(f'Column {i + 1}/{num_cols} complete in {str(iter_time)}')\n",
    "        print(f'Est. completion in {str(iter_time * (num_cols - (i + 1)))}')\n",
    "        \n",
    "    return df_disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = False\n",
    "recalc = False\n",
    "if load:\n",
    "    if recalc:\n",
    "        exec_start = datetime.now()\n",
    "        df_deciles = discretize(df_devs)\n",
    "        print('Writing to file...')\n",
    "        df_deciles.to_csv('wrds_results/ch1/deciles.csv')\n",
    "        print(f'Done in {str(datetime.now() - exec_start)}')\n",
    "    else:\n",
    "        df_deciles = pd.read_csv('wrds_results/ch1/deciles.csv', index_col=0)\n",
    "        print('Deciles loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Errors\n",
    "These are rankings depending on which portion of the normal distribution the data fall into, defined by standard deviation. For given data point $d$, the bins are:   $ d < -2 \\sigma, -2 \\sigma < d < -1.5 \\sigma, \\cdots, 1.5 \\sigma < d < 2 \\sigma, 2 \\sigma < d $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 4.32 GiB for an array with shape (141, 4107776) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-701040d5c0df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mrecalc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mexec_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mdf_stderrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscretize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_devs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Writing to file...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mdf_stderrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wrds_results/ch1/stderr.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-8efb966b1824>\u001b[0m in \u001b[0;36mdiscretize\u001b[1;34m(df_devs, stderr, waiting_period)\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqcut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_devs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mdf_disc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0miter_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0miter_start\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaih2\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3610\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3611\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3612\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3614\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaih2\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3795\u001b[0m                     \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexisting_piece\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3796\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3797\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3799\u001b[0m     def _set_value(\n",
      "\u001b[1;32mc:\\users\\kaih2\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item_mgr\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3754\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3755\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3756\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iset_item_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3757\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3758\u001b[0m         \u001b[1;31m# check if we are modifying a copy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaih2\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_iset_item_mgr\u001b[1;34m(self, loc, value)\u001b[0m\n\u001b[0;32m   3744\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_iset_item_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mslice\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3745\u001b[0m         \u001b[1;31m# when called from _set_item_mgr loc can be anything returned from get_loc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3746\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3747\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaih2\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36miset\u001b[1;34m(self, loc, value)\u001b[0m\n\u001b[0;32m   1085\u001b[0m                     \u001b[0mremoved_blknos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblkno\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1086\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1087\u001b[1;33m                     \u001b[0mblk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblk_locs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1088\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_blklocs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mblk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1089\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaih2\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mdelete\u001b[1;34m(self, loc)\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[0mDelete\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mblock\u001b[0m \u001b[1;32min\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mplace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m         \"\"\"\n\u001b[1;32m--> 366\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmgr_locs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr_locs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdelete\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaih2\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mdelete\u001b[1;34m(arr, obj, axis)\u001b[0m\n\u001b[0;32m   4407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4408\u001b[0m         \u001b[0mslobj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeep\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4409\u001b[1;33m         \u001b[0mnew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4411\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 4.32 GiB for an array with shape (141, 4107776) and data type object"
     ]
    }
   ],
   "source": [
    "load = True\n",
    "recalc = True\n",
    "if load:\n",
    "    if recalc:\n",
    "        exec_start = datetime.now()\n",
    "        df_stderrs = discretize(df_devs, stderr=True)\n",
    "        print('Writing to file...')\n",
    "        df_stderrs.to_csv('wrds_results/ch1/stderr.csv')\n",
    "        print(f'Done in {str(datetime.now() - exec_start)}')\n",
    "    else:\n",
    "        df_stderrs = pd.read_csv('wrds_results/ch1/stderr.csv', index_col=0)\n",
    "        print('Stderrs loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Modeling\n",
    "Finally, for our stability analysis, we'll use the discretizations of our data (both deciles/standard errors) as \"states\" and build a Markov model to analyze how stocks transition between bins. We do this for each transition matrix, as well as one final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_matrix(df, n_quantiles, stderr=False):\n",
    "    transition_matrices = []\n",
    "    periods = [elt[:10] for elt in df.columns]\n",
    "    periods = periods[:-(len(periods) % 20)]\n",
    "    n_decades = len(periods) // 20\n",
    "    for i in range(n_decades):\n",
    "        transition_matrices.append(np.zeros((n_quantiles + 1, n_quantiles + 1)))\n",
    "        \n",
    "    for per_idx in range(len(periods) - 1):\n",
    "        \n",
    "        # start iteration\n",
    "        print(f'Adding period {periods[per_idx]}...')\n",
    "        clear_output(wait=True)\n",
    "        iter_start = datetime.now()\n",
    "        \n",
    "        # build transition matrix\n",
    "        for r_idx, row in df.iterrows():\n",
    "            for c_idx in range(len(row) - 1):\n",
    "                elt = row[c_idx]\n",
    "                next_elt = row[c_idx + 1]\n",
    "                if (not np.isnan(elt)) and (not np.isnan(next_elt)):\n",
    "                    transition_mat[int(elt)][int(next_elt)] += 1\n",
    "            \n",
    "        print(f'Period {periods[per_idx]} complete in {str(datetime.now() - iter_start)}')\n",
    "            \n",
    "    # normalize transition matrix (decades & overall)\n",
    "    overall_matrix = np.zeros((n_quantiles + 1, n_quantiles + 1))\n",
    "    for decade in range(n_decades):\n",
    "        overall_matrix += transition_matrices[decade]\n",
    "        for i in range(n_quantiles + 1):\n",
    "            mat_sum = np.sum(transition_matrices[decade][i])\n",
    "            if mat_sum > 0:\n",
    "                transition_matrices[decade][i] /= mat_sum\n",
    "                \n",
    "    for i in range(n_quantiles + 1):\n",
    "        mat_sum == np.sum(overall_matrix[i])\n",
    "        if mat_sum > 0:\n",
    "            overall_matrix[i] /= mat_sum\n",
    "            \n",
    "    return transition_matrices + [overall_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = False\n",
    "stderr = False\n",
    "if dec:\n",
    "    transition_mats_dec = get_transition_matrix(df_deciles, 10)\n",
    "    for i in range(len(transition_mats_dec) - 1):\n",
    "        np.save(f'wrds_results/ch1/transition_mat_deciles_{1950 + str(i * 10)}', transition_mats_dec[i])\n",
    "    np.save('wrds_results/ch1/transition_mat_deciles_overall', transition_mats_dec[-1])\n",
    "if stderr:\n",
    "    transition_mats_stderr = get_transition_matrix(df_stderrs, 10, stderr=True)\n",
    "    for i in range(len(transition_mats_stderr) - 1):\n",
    "        np.save(f'wrds_results/ch1/transition_mat_stderr_{1950 + str(i * 10)}', transition_mats_stderr[i])\n",
    "    np.save('wrds_results/ch1/transition_mat_stderr_overall', transition_mats_stderr[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Here we display heatmaps of each decade's transition matrix, as well as one final one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
