{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volatility Analysis\n",
    "This notebook adresses Chapter 1 of our research project, namely analyzing the stability of stock volatility (standard deviation of returns) over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import wrds\n",
    "import pandas_market_calendars as mcal\n",
    "from datetime import date, timedelta, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "conn = wrds.Connection(wrds_username='kaihvc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "The goal of this section is to generate a dataset of weekly returns (Fri-Fri) for each stock in our universe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying WRDS for Daily Returns\n",
    "We query the WRDS CRSP stock database for daily returns for any stock with a 2020 $10mil market cap (adjusted for inflation using the GDP deflator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRDS database request & processing\n",
    "# build SQL query using inflation-adjusted mcaps\n",
    "\n",
    "# load inflation-adjusted market caps from file (equivalents of 2020 $10mil)\n",
    "def get_cutoffs(filename):\n",
    "    cutoffs = pd.read_csv(filename)[['observation_date', 'Cutoff Value']]\n",
    "    cutoffs['year'] = pd.to_datetime(cutoffs['observation_date']).apply(lambda x: x.year)\n",
    "    cutoffs = cutoffs.set_index('year').drop(columns='observation_date')\n",
    "    return cutoffs\n",
    "\n",
    "# build query, filtering using inflation-adjusted mcap values\n",
    "def build_query(start_dt, end_dt, adjusted_mcaps):\n",
    "    query = f\"SELECT cusip, date, ret FROM crsp.dsf WHERE date>='{str(start_dt)}' AND date<='{str(end_dt)}' AND (\"\n",
    "    for i, year in enumerate(adjusted_mcaps.loc[start_dt.year:].index):\n",
    "        query += f\"(EXTRACT(YEAR FROM date)={year} AND prc*shrout>={int(adjusted_mcaps.loc[year][0] / 1000)})\"\n",
    "        query += \" OR \" if i < len(adjusted_mcaps.loc[start_dt.year:].index) - 1 else \")\"\n",
    "    return query\n",
    "        \n",
    "# get daily returns\n",
    "def get_returns(start_dt, end_dt, adjusted_mcaps, username='kaihvc'):\n",
    "    conn = wrds.Connection(wrds_username=username)\n",
    "    query = build_query(start_dt, end_dt, adjusted_mcaps)\n",
    "    df = conn.raw_sql(query)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "# we'll use the 1950 inflation-adjusted equivalent of a 2020 $10mil mcap minimum, and do more fine-grained filtering later\n",
    "start_dt = date(1950, 1, 1)\n",
    "end_dt = date(2021, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query returns\n",
    "requery = False\n",
    "if requery:\n",
    "    exec_start = datetime.now()\n",
    "    print(\"Querying WRDS for daily returns...\")\n",
    "    daily_rets = get_returns(start_dt, end_dt, get_cutoffs('inflation_adjustments.csv'))\n",
    "    print(f\"Query complete in {str(datetime.now() - exec_start)}, writing to disk...\")\n",
    "    stage_start = datetime.now()\n",
    "    daily_rets.to_csv('wrds_results/daily_rets_adjusted.csv')\n",
    "    print(f\"Disk write complete in {str(datetime.now() - stage_start)}\")\n",
    "    print(f\"Download complete in {str(datetime.now() - exec_start)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Daily Data to Weekly Data\n",
    "To convert our daily data into weekly data, we'll neeed to delineate the weeks (excluding market holidays, defining minimum days, etc.). We'll then geometrically expand each week's daily returns ($r_d$ for each day $d$) to get the weekly return $r_w$ as:  \n",
    "$r_w = ((1 + r_1) \\cdot (1 + r_2) \\cdot (1 + r_3) \\cdot (1 + r_4) \\cdot (1 + r_5)) - 1$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_process_weekly(df, start_dt, end_dt):\n",
    "    \n",
    "    # takes ~25-30 min on my machine (16G, 2.8GHz) all told; most spent on the aggregation\n",
    "    # group by cusip & weeks, then geometrically expand week groups to get weekly returns\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    grouper = pd.Grouper(key='date', freq='W-MON')\n",
    "    grouped = df.groupby(by=['cusip', grouper], observed=True)\n",
    "    weekly_rets = grouped.agg({'ret': lambda x: (x + 1).product() - 1})\n",
    "    \n",
    "    return weekly_rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weekly returns loaded\n"
     ]
    }
   ],
   "source": [
    "# calculate returns\n",
    "recalc = False\n",
    "if recalc:\n",
    "    if 'daily_rets' not in globals():\n",
    "        print(\"Loading daily returns from file...\")\n",
    "        daily_rets = pd.read_csv('wrds_results/daily_rets_adjusted.csv', index_col=0)\n",
    "    exec_start = datetime.now()\n",
    "    weekly_rets = pd_process_weekly(daily_rets, start_dt, end_dt)\n",
    "    print(f\"Processing complete in {str(datetime.now() - exec_start)}, writing to disk...\")\n",
    "    weekly_rets.to_csv('wrds_results/weekly_rets_adjusted.csv')\n",
    "    print(f'Weekly returns downloaded in {str(datetime.now() - exec_start)}')\n",
    "else:\n",
    "    weekly_rets = pd.read_csv('wrds_results/weekly_rets_adjusted.csv', index_col=[0, 1])\n",
    "    weekly_rets.index = weekly_rets.index.set_levels([weekly_rets.index.levels[0], pd.to_datetime(weekly_rets.index.levels[1])])\n",
    "    print('Weekly returns loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "Once we have weekly returns, we want to calculate their standard deviations over a ~6-month period for each stock, which will be our primary source of data on the stocks' volatility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating 6mo. Standard Deviations\n",
    "Calculating standard deviations is relatively straightforward; the main caveat is generating the periods. We'll be using periods of _approximately_ 26 weeks, with occasional corrections to avoid drift (we want to start as close to the first trading week of the year, or the 26th week of the year, as we can)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_days(start_dt, end_dt):\n",
    "    nyse = mcal.get_calendar('NYSE')\n",
    "    return [day.date() for day in nyse.valid_days(start_date=str(start_dt), \n",
    "                                                end_date=str(end_dt))]\n",
    "\n",
    "def get_weeks(start_dt, end_dt):\n",
    "    weeks = []\n",
    "\n",
    "    # get trading days and divide into weeks, rolling over on the given rollover day\n",
    "    current_week = []\n",
    "    days = get_days(start_dt, end_dt)\n",
    "    for i, day in enumerate(days):\n",
    "        # append to current week\n",
    "        current_week.append(day)\n",
    "\n",
    "        #end of the week check\n",
    "        if i+1<len(days):\n",
    "            next_day = days[i+1]\n",
    "            if next_day.weekday() < day.weekday(): #if the next day's weekday comes before the current, then the next_day is in a new week\n",
    "                weeks.append(current_week)\n",
    "                current_week = []\n",
    "            elif (next_day-day).days > 4: #if the condition above didnt pass, check that >4 days didnt pass as well\n",
    "                weeks.append(current_week)\n",
    "                current_week = []\n",
    "\n",
    "    # append final week, even if it wasn't a full week period\n",
    "    if current_week: weeks.append(current_week)\n",
    "\n",
    "    return weeks\n",
    "\n",
    "def get_periods(weeks):\n",
    "    periods = []\n",
    "    start_date = weeks[0][0]\n",
    "    end_date = None\n",
    "    for i in range(len(weeks)-1):\n",
    "        start = weeks[i][0] #this monday\n",
    "        end = weeks[i][-1]  #this friday\n",
    "\n",
    "        next_start = weeks[i+1][0] #next monday\n",
    "        next_end = weeks[i+1][-1]  #next friday\n",
    "\n",
    "        #if the current friday is in december, and the next weeks friday is in january, this is the end of one period\n",
    "        #the very next week will be the very start of the period\\\n",
    "        if end.month == 12 and next_end.month == 1:\n",
    "            end_date = end\n",
    "            periods.append([start_date,end_date])\n",
    "            start_date = next_start\n",
    "        #if the last day is in june, and the next weeks last day is in july, this is the end of one period\n",
    "        #the very next week will be the very start of the period\n",
    "        if end.month == 6 and next_end.month == 7:\n",
    "            end_date = end\n",
    "            periods.append([start_date,end_date])\n",
    "            start_date = next_start\n",
    "    periods.append([start_date, weeks[i+1][-1]])\n",
    "    for i in periods:\n",
    "        i[1] += timedelta(days=3)\n",
    "    return periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate standard deviations of ~26 week periods; must have min_valid_weeks (default 20) out of 26 to be valid\n",
    "def get_6mo_stds(weekly_rets, periods, min_valid_weeks=20):\n",
    "    \n",
    "    for cusip in weekly_rets.index.levels[0][:30]:\n",
    "        \n",
    "        devs = pd.DataFrame(index=[cusip], columns=['P' + str(i) for i in range(len(periods))])\n",
    "\n",
    "        for i, per in enumerate(periods[:3]):\n",
    "\n",
    "            period_returns = weekly_rets.loc[cusip][per[0]:per[1] - timedelta(days=1)].dropna().values\n",
    "            std = np.std(period_returns) if len(period_returns) >= min_valid_weeks else np.nan\n",
    "\n",
    "            devs.loc[cusip]['P' + str(i)] = std\n",
    "\n",
    "        # if weekly return data exists, append to file\n",
    "        # it's possible that this would be faster to do all in memory, appending to a list & then concatenating\n",
    "        if not weekly_rets['ret'].dropna().empty:\n",
    "            with open('wrds_results/temp_deviations.csv', 'a') as file:\n",
    "                devs.to_csv(file, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 0:00:09.248242\n"
     ]
    }
   ],
   "source": [
    "recalc = True\n",
    "if recalc:\n",
    "    exec_start = datetime.now()\n",
    "    periods = get_periods(get_weeks(start_dt, end_dt))\n",
    "    get_6mo_stds(weekly_rets, periods)\n",
    "    print(f'Done in {str(datetime.now() - exec_start)}')\n",
    "    \n",
    "df_devs = pd.read_csv('wrds_results/temp_deviations.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking Stocks by Volatility\n",
    "Now that we have data for volatility, we want to rank it in order to perform some stability analysis on it. We'll do this using both straight deciles, as well as which portion of the distribution the standard error of each value falls in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deciles\n",
    "These are straight decile rankings - each ranking bin (1-10) contains 10% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discretize into 10 bins\n",
    "def discretize(df_devs, stderr=False, waiting_period=0):\n",
    "    \n",
    "    if waiting_period > 0:\n",
    "        for i, row in df_devs.iterrows():\n",
    "            idxs = row.dropna().index[:waiting_period]\n",
    "            row[idxs] = np.nan\n",
    "            \n",
    "    df_devs = df_devs.dropna(axis='columns', how='all')\n",
    "    \n",
    "    df_disc = pd.DataFrame(columns=df_devs.columns)\n",
    "    for col in df_devs.columns:\n",
    "        if stderr:\n",
    "            intervals = [pd.Interval(elt, elt + 0.5) for elt in np.arange(-2, 2, 0.5)]\n",
    "            intervals = [pd.Interval(-np.inf, -2)] + intervals + [pd.Interval(2, np.inf)]    \n",
    "            intervals = pd.IntervalIndex(intervals)\n",
    "\n",
    "            discretized = pd.cut(df_devs[col], intervals, include_lowest=True)\n",
    "            labels = range(1, len(discretized.categories) + 1)\n",
    "            vals = pd.Series(discretized.rename_categories(labels)).astype(int)\n",
    "        else:\n",
    "            vals = pd.qcut(df_devs[col], 10, labels=range(1, 11))\n",
    "        \n",
    "        df_disc[col] = vals\n",
    "        \n",
    "    return df_disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalc = True\n",
    "if recalc:\n",
    "    df_deciles = discretize(df_devs)\n",
    "    df_deciles.to_csv('wrds_results/ch1/deciles.csv')\n",
    "else:\n",
    "    df_deciles = pd.read_csv('wrds_results/ch1/deciles.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Errors\n",
    "These are rankings depending on which portion of the normal distribution the data fall into, defined by standard deviation. For given data point $d$, the bins are:   $ d < -2 \\sigma, -2 \\sigma < d < -1.5 \\sigma, \\cdots, 1.5 \\sigma < d < 2 \\sigma, 2 \\sigma < d $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalc = True\n",
    "if recalc:\n",
    "    df_stderrs = discretize(df_devs, stderr=True)\n",
    "    df_stderrs.to_csv('wrds_results/ch1/stderrs.csv')\n",
    "else:\n",
    "    df_stderrs = pd.read_csv('wrds_results/ch1/deciles.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Modeling\n",
    "Finally, for our stability analysis, we'll use the discretizations of our data (both deciles/standard errors) as \"states\" and build a Markov model to analyze how stocks transition between bins. We do this for each transition matrix, as well as one final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_matrix(df, n_quantiles, stderr=False):\n",
    "    transition_matrices = []\n",
    "    periods = [elt[:10] for elt in df.columns]\n",
    "    periods = periods[:-(len(periods) % 20)]\n",
    "    n_decades = len(periods) // 20\n",
    "    for i in range(n_decades):\n",
    "        transition_matrices.append(np.zeros((n_quantiles + 1, n_quantiles + 1)))\n",
    "        \n",
    "    for per_idx in range(len(periods) - 1):\n",
    "        \n",
    "        # start iteration\n",
    "        print(f'Adding period {periods[per_idx]}...')\n",
    "        clear_output(wait=True)\n",
    "        iter_start = datetime.now()\n",
    "        \n",
    "        # build transition matrix\n",
    "        for r_idx, row in df.iterrows():\n",
    "            for c_idx in range(len(row) - 1):\n",
    "                elt = row[c_idx]\n",
    "                next_elt = row[c_idx + 1]\n",
    "                if (not np.isnan(elt)) and (not np.isnan(next_elt)):\n",
    "                    transition_mat[int(elt)][int(next_elt)] += 1\n",
    "            \n",
    "        print(f'Period {periods[per_idx]} complete in {str(datetime.now() - iter_start)}')\n",
    "            \n",
    "    # normalize transition matrix (decades & overall)\n",
    "    overall_matrix = np.zeros((n_quantiles + 1, n_quantiles + 1))\n",
    "    for decade in range(n_decades):\n",
    "        overall_matrix += transition_matrices[decade]\n",
    "        for i in range(n_quantiles + 1):\n",
    "            mat_sum = np.sum(transition_matrices[decade][i])\n",
    "            if mat_sum > 0:\n",
    "                transition_matrices[decade][i] /= mat_sum\n",
    "                \n",
    "    for i in range(n_quantiles + 1):\n",
    "        mat_sum == np.sum(overall_matrix[i])\n",
    "        if mat_sum > 0:\n",
    "            overall_matrix[i] /= mat_sum\n",
    "            \n",
    "    return transition_matrices + [overall_matrix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Here we display heatmaps of each decade's transition matrix, as well as one final one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
