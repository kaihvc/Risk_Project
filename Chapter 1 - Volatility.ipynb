{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volatility Analysis\n",
    "This notebook adresses Chapter 1 of our research project, namely analyzing the stability of stock volatility (standard deviation of returns) over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import wrds\n",
    "import pandas_market_calendars as mcal\n",
    "from datetime import date, timedelta, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "conn = wrds.Connection(wrds_username='kaihvc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "The goal of this section is to generate a dataset of weekly returns (Fri-Fri) for each stock in our universe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying WRDS for Daily Returns\n",
    "We query the WRDS CRSP stock database for daily returns for any stock with a 2020 $10mil market cap (adjusted for inflation using the GDP deflator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRDS database request & processing\n",
    "# build SQL query using inflation-adjusted mcaps\n",
    "\n",
    "# load inflation-adjusted market caps from file (equivalents of 2020 $10mil)\n",
    "def get_cutoffs(filename):\n",
    "    cutoffs = pd.read_csv(filename)[['observation_date', 'Cutoff Value']]\n",
    "    cutoffs['year'] = pd.to_datetime(cutoffs['observation_date']).apply(lambda x: x.year)\n",
    "    cutoffs = cutoffs.set_index('year').drop(columns='observation_date')\n",
    "    return cutoffs\n",
    "\n",
    "# build query, filtering using inflation-adjusted mcap values\n",
    "def build_query(start_dt, end_dt, adjusted_mcaps):\n",
    "    query = f\"SELECT cusip, date, ret FROM crsp.dsf WHERE date>='{str(start_dt)}' AND date<='{str(end_dt)}' AND (\"\n",
    "    for i, year in enumerate(adjusted_mcaps.loc[start_dt.year:].index):\n",
    "        query += f\"(EXTRACT(YEAR FROM date)={year} AND prc*shrout>={int(adjusted_mcaps.loc[year][0] / 1000)})\"\n",
    "        query += \" OR \" if i < len(adjusted_mcaps.loc[start_dt.year:].index) - 1 else \")\"\n",
    "    return query\n",
    "        \n",
    "# get daily returns\n",
    "def get_returns(start_dt, end_dt, adjusted_mcaps, username='kaihvc'):\n",
    "    conn = wrds.Connection(wrds_username=username)\n",
    "    query = build_query(start_dt, end_dt, adjusted_mcaps)\n",
    "    df = conn.raw_sql(query)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "# we'll use the 1950 inflation-adjusted equivalent of a 2020 $10mil mcap minimum, and do more fine-grained filtering later\n",
    "start_dt = date(1950, 1, 1)\n",
    "end_dt = date(2021, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query returns\n",
    "requery = False\n",
    "if requery:\n",
    "    exec_start = datetime.now()\n",
    "    print(\"Querying WRDS for daily returns...\")\n",
    "    daily_rets = get_returns(start_dt, end_dt, get_cutoffs('inflation_adjustments.csv'))\n",
    "    print(f\"Query complete in {str(datetime.now() - exec_start)}, writing to disk...\")\n",
    "    stage_start = datetime.now()\n",
    "    daily_rets.to_csv('wrds_results/daily_rets_adjusted.csv')\n",
    "    print(f\"Disk write complete in {str(datetime.now() - stage_start)}\")\n",
    "    print(f\"Download complete in {str(datetime.now() - exec_start)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Daily Data to Weekly Data\n",
    "To convert our daily data into weekly data, we'll neeed to delineate the weeks (excluding market holidays, defining minimum days, etc.). We'll then geometrically expand each week's daily returns ($r_d$ for each day $d$) to get the weekly return $r_w$ as:  \n",
    "$r_w = ((1 + r_1) \\cdot (1 + r_2) \\cdot (1 + r_3) \\cdot (1 + r_4) \\cdot (1 + r_5)) - 1$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_process_weekly(df, start_dt, end_dt):\n",
    "    \n",
    "    # takes ~25-30 min on my machine (16G, 2.8GHz) all told; most spent on the aggregation\n",
    "    # group by cusip & weeks, then geometrically expand week groups to get weekly returns\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    grouper = pd.Grouper(key='date', freq='W-MON')\n",
    "    grouped = df.groupby(by=['cusip', grouper], observed=True)\n",
    "    weekly_rets = grouped.agg({'ret': lambda x: (x + 1).product() - 1})\n",
    "    \n",
    "    return weekly_rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weekly returns loaded\n"
     ]
    }
   ],
   "source": [
    "# calculate returns\n",
    "recalc = False\n",
    "if recalc:\n",
    "    if 'daily_rets' not in globals():\n",
    "        print(\"Loading daily returns from file...\")\n",
    "        daily_rets = pd.read_csv('wrds_results/daily_rets_adjusted.csv', index_col=0)\n",
    "    exec_start = datetime.now()\n",
    "    weekly_rets = pd_process_weekly(daily_rets, start_dt, end_dt)\n",
    "    print(f\"Processing complete in {str(datetime.now() - exec_start)}, writing to disk...\")\n",
    "    weekly_rets.to_csv('wrds_results/weekly_rets_adjusted.csv')\n",
    "    print(f'Weekly returns downloaded in {str(datetime.now() - exec_start)}')\n",
    "else:\n",
    "    weekly_rets = pd.read_csv('wrds_results/weekly_rets_adjusted.csv', index_col=[0, 1])\n",
    "    weekly_rets.index = weekly_rets.index.set_levels([weekly_rets.index.levels[0], pd.to_datetime(weekly_rets.index.levels[1])])\n",
    "    print('Weekly returns loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "Once we have weekly returns, we want to calculate their standard deviations over a ~6-month period for each stock, which will be our primary source of data on the stocks' volatility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating 6mo. Standard Deviations\n",
    "Calculating standard deviations is relatively straightforward; the main caveat is generating the periods. We'll be using periods of _approximately_ 26 weeks, with occasional corrections to avoid drift (we want to start as close to the first trading week of the year, or the 26th week of the year, as we can)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_days(start_dt, end_dt):\n",
    "    nyse = mcal.get_calendar('NYSE')\n",
    "    return [day.date() for day in nyse.valid_days(start_date=str(start_dt), \n",
    "                                                end_date=str(end_dt))]\n",
    "\n",
    "def get_weeks(start_dt, end_dt):\n",
    "    weeks = []\n",
    "\n",
    "    # get trading days and divide into weeks, rolling over on the given rollover day\n",
    "    current_week = []\n",
    "    days = get_days(start_dt, end_dt)\n",
    "    for i, day in enumerate(days):\n",
    "        # append to current week\n",
    "        current_week.append(day)\n",
    "\n",
    "        #end of the week check\n",
    "        if i+1<len(days):\n",
    "            next_day = days[i+1]\n",
    "            if next_day.weekday() < day.weekday(): #if the next day's weekday comes before the current, then the next_day is in a new week\n",
    "                weeks.append(current_week)\n",
    "                current_week = []\n",
    "            elif (next_day-day).days > 4: #if the condition above didnt pass, check that >4 days didnt pass as well\n",
    "                weeks.append(current_week)\n",
    "                current_week = []\n",
    "\n",
    "    # append final week, even if it wasn't a full week period\n",
    "    if current_week: weeks.append(current_week)\n",
    "\n",
    "    return weeks\n",
    "\n",
    "def get_periods(weeks):\n",
    "    periods = []\n",
    "    start_date = weeks[0][0]\n",
    "    end_date = None\n",
    "    for i in range(len(weeks)-1):\n",
    "        start = weeks[i][0] #this monday\n",
    "        end = weeks[i][-1]  #this friday\n",
    "\n",
    "        next_start = weeks[i+1][0] #next monday\n",
    "        next_end = weeks[i+1][-1]  #next friday\n",
    "\n",
    "        #if the current friday is in december, and the next weeks friday is in january, this is the end of one period\n",
    "        #the very next week will be the very start of the period\\\n",
    "        if end.month == 12 and next_end.month == 1:\n",
    "            end_date = end\n",
    "            periods.append([start_date,end_date])\n",
    "            start_date = next_start\n",
    "        #if the last day is in june, and the next weeks last day is in july, this is the end of one period\n",
    "        #the very next week will be the very start of the period\n",
    "        if end.month == 6 and next_end.month == 7:\n",
    "            end_date = end\n",
    "            periods.append([start_date,end_date])\n",
    "            start_date = next_start\n",
    "    periods.append([start_date, weeks[i+1][-1]])\n",
    "    for i in periods:\n",
    "        i[1] += timedelta(days=3)\n",
    "    return periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate standard deviations of ~26 week periods; \n",
    "# must have min_valid_weeks (default 20) out of 26 to be valid\n",
    "def get_6mo_stds(weekly_rets, periods, min_valid_weeks=20):\n",
    "    \n",
    "    num_pers = len(periods)\n",
    "    devs_df_list = []\n",
    "    devs = pd.DataFrame(index=weekly_rets.index.levels[0], columns=['P' + str(i) for i in range(len(periods))])\n",
    "\n",
    "    for i, per in enumerate(periods):\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        iter_start = datetime.now()\n",
    "\n",
    "        filter_counts = weekly_rets.loc[(slice(None), \n",
    "                                    slice(per[0], \n",
    "                                          per[1] - timedelta(days=1))), \n",
    "                                   :].groupby('cusip').count().rename(columns={'ret': 'count'})\n",
    "        agg_devs = weekly_rets.loc[(slice(None), \n",
    "                                    slice(per[0], \n",
    "                                          per[1] - timedelta(days=1))), \n",
    "                                   :].groupby('cusip').agg(np.std).loc[filter_counts['count'] >= min_valid_weeks]\n",
    "        devs.loc[agg_devs.index, 'P' + str(i)] = agg_devs.values.flatten()\n",
    "        devs_df_list.append(devs)\n",
    "                \n",
    "        iter_time = datetime.now() - iter_start\n",
    "        print(f'Cusip {i + 1}/{num_pers} complete in {str(iter_time)}')\n",
    "        print(f'Est. completion in {str(iter_time * (num_pers - (i + 1)))}')\n",
    "        \n",
    "    try:\n",
    "        devs = pd.concat(devs_df_list)\n",
    "        return devs\n",
    "    except MemoryError:\n",
    "        print('Memory error encountered - attempting to remedy by dropping weekly_rets')\n",
    "        del weekly_rets\n",
    "        try:\n",
    "            devs = pd.concat(devs_df_list)\n",
    "            return devs\n",
    "        except MemoryError:\n",
    "            print('Remedy failed - reload weekly_rets and buy more memory!')\n",
    "            return -1\n",
    "    \n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviations loaded\n"
     ]
    }
   ],
   "source": [
    "recalc = False\n",
    "if recalc:\n",
    "    exec_start = datetime.now()\n",
    "    periods = get_periods(get_weeks(start_dt, end_dt))\n",
    "    df_devs = get_6mo_stds(weekly_rets, periods)\n",
    "    print('Writing to file...')\n",
    "    df_devs.to_csv('wrds_results/deviations.csv')\n",
    "    print(f'Done in {str(datetime.now() - exec_start)}')\n",
    "else:\n",
    "    df_devs = pd.read_csv('wrds_results/deviations.csv',index_col=0)\n",
    "    print('Standard deviations loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking Stocks by Volatility\n",
    "Now that we have data for volatility, we want to rank it in order to perform some stability analysis on it. We'll do this using both straight deciles, as well as which portion of the distribution the standard error of each value falls in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deciles\n",
    "These are straight decile rankings - each ranking bin (1-10) contains 10% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discretize into 10 bins\n",
    "def discretize(df_devs, stderr=False, waiting_period=0):\n",
    "    \n",
    "    print('Beginning discretization...')\n",
    "    \n",
    "    if waiting_period > 0:\n",
    "        for i, row in df_devs.iterrows():\n",
    "            idxs = row.dropna().index[:waiting_period]\n",
    "            row[idxs] = np.nan\n",
    "            \n",
    "    df_devs = df_devs.dropna(axis='columns', how='all')\n",
    "    \n",
    "    df_disc = pd.DataFrame(columns=df_devs.columns)\n",
    "    num_cols = len(df_devs.columns)\n",
    "    for i, col in enumerate(df_devs.columns):\n",
    "        iter_start = datetime.now()\n",
    "        clear_output(wait=True)\n",
    "        if stderr:\n",
    "            intervals = [pd.Interval(elt, elt + 0.5) for elt in np.arange(-2, 2, 0.5)]\n",
    "            intervals = [pd.Interval(-np.inf, -2)] + intervals + [pd.Interval(2, np.inf)]    \n",
    "            intervals = pd.IntervalIndex(intervals)\n",
    "            \n",
    "            stderr_col = (df_devs[col] - np.nanmean(df_devs[col])) / np.nanstd(df_devs[col])\n",
    "\n",
    "            ''' \n",
    "                note: it's possible that not every period will include 10 categories, since this\n",
    "                bins by set intervals rather than quantiles of the data itself.\n",
    "            ''' \n",
    "            discretized = pd.cut(stderr_col, intervals, include_lowest=True)\n",
    "            labels = range(1, len(discretized.cat.categories) + 1)\n",
    "            vals = discretized.cat.rename_categories(labels)\n",
    "        else:\n",
    "            # however, this method *should* always include 10 categories for each period\n",
    "            vals = pd.qcut(df_devs[col], 10, labels=range(1, 11))\n",
    "        \n",
    "        df_disc[col] = vals\n",
    "        \n",
    "        iter_time = datetime.now() - iter_start\n",
    "        print(f'Column {i + 1}/{num_cols} complete in {str(iter_time)}')\n",
    "        print(f'Est. completion in {str(iter_time * (num_cols - (i + 1)))}')\n",
    "        \n",
    "    return df_disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to file...\n",
      "Done in 0:01:16.727326\n"
     ]
    }
   ],
   "source": [
    "recalc = False\n",
    "if recalc:\n",
    "    exec_start = datetime.now()\n",
    "    df_deciles = discretize(df_devs)\n",
    "    print('Writing to file...')\n",
    "    df_deciles.to_csv('wrds_results/ch1/deciles.csv')\n",
    "    print(f'Done in {str(datetime.now() - exec_start)}')\n",
    "else:\n",
    "    df_deciles = pd.read_csv('wrds_results/ch1/deciles.csv', index_col=0)\n",
    "    print('Deciles loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Errors\n",
    "These are rankings depending on which portion of the normal distribution the data fall into, defined by standard deviation. For given data point $d$, the bins are:   $ d < -2 \\sigma, -2 \\sigma < d < -1.5 \\sigma, \\cdots, 1.5 \\sigma < d < 2 \\sigma, 2 \\sigma < d $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 4.35 GiB for an array with shape (4107776, 142) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recalc:\n\u001b[0;32m      3\u001b[0m     exec_start \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m----> 4\u001b[0m     df_stderrs \u001b[38;5;241m=\u001b[39m \u001b[43mdiscretize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_devs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWriting to file...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m     df_stderrs\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwrds_results/ch1/stderr.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36mdiscretize\u001b[1;34m(df_devs, stderr, waiting_period)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# however, this method *should* always include 10 categories for each period\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     vals \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mqcut(df_devs[col], \u001b[38;5;241m10\u001b[39m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11\u001b[39m))\n\u001b[1;32m---> 36\u001b[0m df_disc[col] \u001b[38;5;241m=\u001b[39m vals\n\u001b[0;32m     38\u001b[0m iter_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m iter_start\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m complete in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(iter_time)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3655\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3652\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   3653\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3654\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3832\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3823\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3824\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   3825\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3830\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   3831\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3832\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3834\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3835\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   3836\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   3837\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[0;32m   3838\u001b[0m     ):\n\u001b[0;32m   3839\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   3840\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:4522\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4509\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sanitize_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ArrayLike:\n\u001b[0;32m   4510\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4511\u001b[0m \u001b[38;5;124;03m    Ensures new columns (which go into the BlockManager as new blocks) are\u001b[39;00m\n\u001b[0;32m   4512\u001b[0m \u001b[38;5;124;03m    always copied and converted into an array.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4520\u001b[0m \u001b[38;5;124;03m    numpy.ndarray or ExtensionArray\u001b[39;00m\n\u001b[0;32m   4521\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4522\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_valid_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4524\u001b[0m     \u001b[38;5;66;03m# We should never get here with DataFrame value\u001b[39;00m\n\u001b[0;32m   4525\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Series):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3904\u001b[0m, in \u001b[0;36mDataFrame._ensure_valid_index\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   3901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3902\u001b[0m     index_copy\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m-> 3904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_copy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnan\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\base.py:101\u001b[0m, in \u001b[0;36mDataManager.reindex_axis\u001b[1;34m(self, new_index, axis, fill_value, consolidate, only_slice)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;03mConform data manager to new index.\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m new_index, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mreindex(new_index)\n\u001b[1;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconsolidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monly_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:692\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    685\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[0;32m    686\u001b[0m         indexer,\n\u001b[0;32m    687\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[0;32m    688\u001b[0m         only_slice\u001b[38;5;241m=\u001b[39monly_slice,\n\u001b[0;32m    689\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39muse_na_proxy,\n\u001b[0;32m    690\u001b[0m     )\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 692\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    693\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[0;32m    694\u001b[0m             indexer,\n\u001b[0;32m    695\u001b[0m             axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    696\u001b[0m             fill_value\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    697\u001b[0m                 fill_value \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mfill_value\n\u001b[0;32m    698\u001b[0m             ),\n\u001b[0;32m    699\u001b[0m         )\n\u001b[0;32m    700\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m    701\u001b[0m     ]\n\u001b[0;32m    703\u001b[0m new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m    704\u001b[0m new_axes[axis] \u001b[38;5;241m=\u001b[39m new_axis\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:693\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    685\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[0;32m    686\u001b[0m         indexer,\n\u001b[0;32m    687\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[0;32m    688\u001b[0m         only_slice\u001b[38;5;241m=\u001b[39monly_slice,\n\u001b[0;32m    689\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39muse_na_proxy,\n\u001b[0;32m    690\u001b[0m     )\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    692\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 693\u001b[0m         \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m            \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    700\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m    701\u001b[0m     ]\n\u001b[0;32m    703\u001b[0m new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m    704\u001b[0m new_axes[axis] \u001b[38;5;241m=\u001b[39m new_axis\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:1121\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1119\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 1121\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43malgos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[0;32m   1123\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m#  this assertion\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m new_mgr_locs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[0;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\array_algos\\take.py:158\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    156\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[0;32m    161\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[0;32m    162\u001b[0m )\n\u001b[0;32m    163\u001b[0m func(arr, indexer, out, fill_value)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 4.35 GiB for an array with shape (4107776, 142) and data type object"
     ]
    }
   ],
   "source": [
    "recalc = True\n",
    "if recalc:\n",
    "    exec_start = datetime.now()\n",
    "    df_stderrs = discretize(df_devs, stderr=True)\n",
    "    print('Writing to file...')\n",
    "    df_stderrs.to_csv('wrds_results/ch1/stderr.csv')\n",
    "    print(f'Done in {str(datetime.now() - exec_start)}')\n",
    "else:\n",
    "    df_stderrs = pd.read_csv('wrds_results/ch1/stderr.csv', index_col=0)\n",
    "    print('Stderrs loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Modeling\n",
    "Finally, for our stability analysis, we'll use the discretizations of our data (both deciles/standard errors) as \"states\" and build a Markov model to analyze how stocks transition between bins. We do this for each transition matrix, as well as one final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_matrix(df, n_quantiles, stderr=False):\n",
    "    transition_matrices = []\n",
    "    periods = [elt[:10] for elt in df.columns]\n",
    "    periods = periods[:-(len(periods) % 20)]\n",
    "    n_decades = len(periods) // 20\n",
    "    for i in range(n_decades):\n",
    "        transition_matrices.append(np.zeros((n_quantiles + 1, n_quantiles + 1)))\n",
    "        \n",
    "    for per_idx in range(len(periods) - 1):\n",
    "        \n",
    "        # start iteration\n",
    "        print(f'Adding period {periods[per_idx]}...')\n",
    "        clear_output(wait=True)\n",
    "        iter_start = datetime.now()\n",
    "        \n",
    "        # build transition matrix\n",
    "        for r_idx, row in df.iterrows():\n",
    "            for c_idx in range(len(row) - 1):\n",
    "                elt = row[c_idx]\n",
    "                next_elt = row[c_idx + 1]\n",
    "                if (not np.isnan(elt)) and (not np.isnan(next_elt)):\n",
    "                    transition_mat[int(elt)][int(next_elt)] += 1\n",
    "            \n",
    "        print(f'Period {periods[per_idx]} complete in {str(datetime.now() - iter_start)}')\n",
    "            \n",
    "    # normalize transition matrix (decades & overall)\n",
    "    overall_matrix = np.zeros((n_quantiles + 1, n_quantiles + 1))\n",
    "    for decade in range(n_decades):\n",
    "        overall_matrix += transition_matrices[decade]\n",
    "        for i in range(n_quantiles + 1):\n",
    "            mat_sum = np.sum(transition_matrices[decade][i])\n",
    "            if mat_sum > 0:\n",
    "                transition_matrices[decade][i] /= mat_sum\n",
    "                \n",
    "    for i in range(n_quantiles + 1):\n",
    "        mat_sum == np.sum(overall_matrix[i])\n",
    "        if mat_sum > 0:\n",
    "            overall_matrix[i] /= mat_sum\n",
    "            \n",
    "    return transition_matrices + [overall_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = False\n",
    "stderr = False\n",
    "if dec:\n",
    "    transition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Here we display heatmaps of each decade's transition matrix, as well as one final one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
