{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Summary\n",
    "This research project was designed to analyze the stability of various financial risk metrics between 6-month periods. The specific metrics analyzed were:\n",
    "- Volatility of returns (standard deviation of weekly returns)\n",
    "- Pairwise correlations (correlation between the weekly returns of each pair of stocks in the universe)\n",
    "- Market exposure (each stock’s market beta; this was calculated using the correlation between the weekly returns of the market and the particular stock, as well as the standard deviation of both sets of weekly returns  \n",
    "\n",
    "Once calculated, each risk metric was discretized (separated into bins) for each 6-month period. Two methods for discretization were used:\n",
    "- Deciles (separating each period’s values into 10 equal-sized bins), and\n",
    "- Standard errors (separating each period’s values into 10 bins based on their normalized values. Each bin had a width of 0.5 period standard deviations, with boundaries at: (explain))\n",
    "\n",
    "**Chapter 1: Volatility**\n",
    "\n",
    "*Data collection*  \n",
    "To calculate volatility, we queried daily return data from the WRDS CRSP database. We selected all stocks from 1950 onward with a market capitalization of at least the equivalent of $10 million 2020 USD. We adjusted for inflation using the GDP deflator, using one dollar value for each year.\n",
    "\n",
    "*Metric calculation*  \n",
    "The daily returns we obtained were then combined into weekly returns through geometric expansion <formula>. For each stock, weeks with less than 5 days due to market holidays were kept; any other weeks with missing data were dropped. \n",
    "\n",
    "These weekly returns were then separated into periods of approximately 26 weeks, with occasional adjustments to ensure each period started in the first week of either January or July. The standard deviation of weekly returns for each 26-week period was calculated.\n",
    "\n",
    "*Discretization*  \n",
    "Next, the stocks in each period were separated into 10 bins based on their standard deviations. This separation was done using both deciles (separating the stocks into 10 bins of equal size) and standard errors (separating the stocks by the standard error of their standard deviations, and binning them into 10 bins based on those values).\n",
    "\n",
    "*Markov modeling*  \n",
    "Finally, the stability of both discretizations was analyzed using Markov modeling. A 10x10 transition matrix was created, and for each stock, transitions between consecutive periods were recorded in the matrix. For each two consecutive periods A and B, if a stock is in bin i in period A and j in period B, then cell (i, j) of the matrix is incremented by one. This is repeated for each stock in the universe. Afterwards, each row of the matrix is normalized (scaled to sum to 1). This means that each row i of the matrix contains the probabilities (in each element j) that a given stock will move from bin i to bin j.\n",
    "\n",
    "*Average values*  \n",
    "During the Markov modeling process, the average volatility value for each cell in the transition matrix is also calculated. For each transition between period A and B, the volatility value for period A is added to one matrix, and the value for period B is added to another. After the enumeration of stocks & periods, before the normalization of the transition matrix, both matrices (A values and B values) are divided element-wise by the cells of the transition matrix, providing average values.\n",
    "\n",
    "**Chapter 2: Correlations**\n",
    "\n",
    "*Data collection*  \n",
    "No further data was collected for this section.\n",
    "\n",
    "*Metric calculation*  \n",
    "To calculate pairwise stock correlations, we used the weekly returns calculated in the previous section. For each of the same 26-week periods, we calculated the correlation between the weekly returns of each pair of stocks.\n",
    "\n",
    "*Discretization*  \n",
    "We discretized the correlations in the same way as the previous chapter, using both deciles and standard errors\n",
    "\n",
    "*Markov modeling*  \n",
    "We performed Markov modeling in the same way as the previous chapter, for both decile and standard error discretizations.\n",
    "\n",
    "*Average values*  \n",
    "The average correlation was also calculated for each transition matrix cell in the same way as the previous section.\n",
    "\n",
    "**Chapter 3: Beta**\n",
    "\n",
    "*Data collection*  \n",
    "We collected daily market index return data from the WRDS CRSP database to reference for market exposure. This data was combined into 26-week periods \n",
    "\n",
    "*Metric calculation*  \n",
    "To calculate pairwise stock correlations, we used the weekly returns calculated in the previous section. For each of the same 26-week periods, we calculated the correlation between the weekly returns of each pair of stocks.\n",
    "\n",
    "*Discretization*  \n",
    "We discretized the correlations in the same way as the previous chapter, using both deciles and standard errors\n",
    "\n",
    "*Markov modeling*  \n",
    "We performed Markov modeling in the same way as the previous chapter, for both decile and standard error discretizations.\n",
    "\n",
    "*Average values*  \n",
    "The average correlation was also calculated for each transition matrix cell in the same way as the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_decades = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "### Chapter 1: Volatility\n",
    "#### Transition matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transition matrices\n",
    "for i in range(n_decades):\n",
    "    mat = np.load(f'wrds_results/ch1/transition_mat_deciles_{1950 + i * 10}.npy')\n",
    "    sns.heatmap(mat, ax=axes[i][0])\n",
    "    mat = np.load(f'wrds_results/ch1/transition_mat_stderr_{1950 + i * 10}.npy')\n",
    "    sns.heatmap(mat, ax=axes[i][1])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
